import multiprocessing
from tqdm import tqdm
from functools import partial
import datasets


raise NotImplementedError("This script is generated by LLM, not checked yet.")

def process_example(example, example_id):
    """Process a single example for multiprocessing."""
    question = example["chosen"][0]["content"]
    return {
        "data_source": "skywork_raw_v2",
        "question": question,
        "response1": " ".join([turn["content"] for turn in example["chosen"] if turn["role"] == "assistant"]),
        "response2": " ".join([turn["content"] for turn in example["rejected"] if turn["role"] == "assistant"]),
        "answer": "1",
        "extra_info": {'index': example_id, 'split': 'train'}
    }

def transform_to_skywork_raw_v2_from_raw_v1(src_fn, trg_fn, num_processes=None):
    """Transform dataset with multiprocessing."""
    if num_processes is None:
        # Use number of available CPUs by default, but leave one free
        num_processes = max(1, multiprocessing.cpu_count() - 1)
    
    print(f"Loading dataset from {src_fn}...")
    src_ds = datasets.load_from_disk(dataset_path=src_fn)["train"]
    total_examples = len(src_ds)
    print(f"Processing {total_examples} examples with {num_processes} processes")
    
    # Create chunks of data to process in parallel
    chunk_size = 1000  # Process in batches to avoid memory issues
    
    all_results = []
    
    with multiprocessing.Pool(processes=num_processes) as pool:
        for i in range(0, total_examples, chunk_size):
            end_idx = min(i + chunk_size, total_examples)
            chunk_indices = list(range(i, end_idx))
            chunk_examples = [src_ds[idx] for idx in chunk_indices]
            
            # Process the chunk in parallel
            process_func = partial(process_example)
            chunk_results = list(tqdm(
                pool.starmap(process_func, zip(chunk_examples, chunk_indices)),
                total=len(chunk_indices),
                desc=f"Processing examples {i}-{end_idx-1}"
            ))
            
            all_results.extend(chunk_results)
    
    print(f"Processed {len(all_results)} examples. Converting to DataFrame...")
    trg_df = pd.DataFrame(all_results)
    
    print(f"Saving to {trg_fn}")
    trg_df.to_parquet(trg_fn)
    print("Transformation complete!")


if __name__ == '__main__':
    src_fn = "/mnt/jiaxin/skywork-data//skywork_raw"
    trg_fn = "/mnt/jiaxin/skywork-data/skywork_raw_v2/train.parquet"
    # Use 8 processes for parallel processing - adjust based on your machine's capabilities
    transform_to_skywork_raw_v2_from_raw_v1(src_fn, trg_fn, num_processes=8)
    # src_fn = "/mnt/jiaxin/skywork-data/skywork.parquet"
    # trg_fn = "/mnt/jiaxin/skywork-data/skywork_raw_v2/test.parquet"
    # transform_to_skywork_raw_v2(src_fn, trg_fn)